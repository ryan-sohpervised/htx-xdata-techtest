{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c11a4971",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook is for Model Prediction Task 1, where I build a predictive algorithm to determine the factors affecting prices of residential properties in Singapore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614a3d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import optuna\n",
    "import xgboost\n",
    "import shap\n",
    "\n",
    "import sklearn\n",
    "sklearn.set_config(transform_output=\"pandas\")\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, TargetEncoder\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6573aa",
   "metadata": {},
   "source": [
    "# Data Import\n",
    "I import and merge the different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a7ffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "\n",
    "raw_data_folder = Path('../data/HDB')\n",
    "\n",
    "price_approval_1990_1993 = pd.read_csv(raw_data_folder / 'resale-flat-prices-based-on-approval-date-1990-1999.csv')\n",
    "price_approval_2000_2012 = pd.read_csv(raw_data_folder / 'resale-flat-prices-based-on-approval-date-2000-feb-2012.csv')\n",
    "price_reg_2015_2016 = pd.read_csv(raw_data_folder / 'resale-flat-prices-based-on-registration-date-from-jan-2015-to-dec-2016.csv')\n",
    "price_reg_2017 = pd.read_csv(raw_data_folder / 'resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv')\n",
    "price_reg_2012_2014 = pd.read_csv(raw_data_folder / 'resale-flat-prices-based-on-registration-date-from-mar-2012-to-dec-2014.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c91af32",
   "metadata": {},
   "source": [
    "I inspect the differences in the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d24665",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(price_approval_1990_1993.head())\n",
    "display(price_approval_2000_2012.head())\n",
    "display(price_reg_2012_2014.head())\n",
    "display(price_reg_2015_2016.head())\n",
    "display(price_reg_2017.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613b653f",
   "metadata": {},
   "source": [
    "In general, the dataframes share most of their columns. The earlier years are missing remaining lease column, whereas the later years have the remaining lease column but in different format. I will generate the remaining lease column for the earlier year dataframes. I will also regenerate the remaining lease period for the later two dataframes to standardise the method of calculating the lease. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd94c4e",
   "metadata": {},
   "source": [
    "## Join all dataframes, except for lease column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacae050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that all dataframes have the same column types\n",
    "\n",
    "# Get reference dataframe column types\n",
    "reference_df = price_approval_1990_1993\n",
    "reference_dtypes = reference_df.dtypes\n",
    "columns_to_join=reference_df.columns\n",
    "\n",
    "all_consistent = True\n",
    "dataframes = {\n",
    "    'price_approval_2000_2012': price_approval_2000_2012,\n",
    "    'price_reg_2012_2014': price_reg_2012_2014, \n",
    "    'price_reg_2015_2016': price_reg_2015_2016,\n",
    "    'price_reg_2017': price_reg_2017\n",
    "}\n",
    "\n",
    "for df_name, df in dataframes.items():\n",
    "    print(f\"Checking {df_name}:\")\n",
    "    \n",
    "    type_mismatches = []\n",
    "    for col in columns_to_join:\n",
    "        if col in df.columns:\n",
    "            ref_type = reference_dtypes[col]\n",
    "            curr_type = df[col].dtype\n",
    "            if ref_type != curr_type:\n",
    "                type_mismatches.append(f\"{col}:{curr_type} instead of {ref_type}\")\n",
    "                all_consistent = False\n",
    "    \n",
    "    if type_mismatches:\n",
    "        for mismatch in type_mismatches:\n",
    "            print(mismatch)\n",
    "    else:\n",
    "        print(f\"All column types match reference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9d8d77",
   "metadata": {},
   "source": [
    "Since all the other dataframes are the same, I will align the earliest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b34a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the column type of resale price in the original dataframe to be consistent with other dataframes\n",
    "price_approval_1990_1993['resale_price'] = price_approval_1990_1993['resale_price'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05219e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns that are common across all dataframes\n",
    "all_dataframes = [price_approval_1990_1993] + list(dataframes.values())\n",
    "common_columns = list(price_approval_1990_1993.columns)\n",
    "\n",
    "# Concat dataframes together only for columns that are common across all dataframes\n",
    "combined_df = pd.concat(\n",
    "    [df[common_columns] for df in all_dataframes],\n",
    ")\n",
    "\n",
    "print(f\"\\nCombined dataframe shape: {combined_df.shape}\")\n",
    "display(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6c3225",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee643c1",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b785bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values in combined dataframe\n",
    "combined_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cdf3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change all string to upper case\n",
    "for col in combined_df.select_dtypes(include=['object']).columns:\n",
    "    combined_df[col] = combined_df[col].str.upper()\n",
    "\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea88e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in combined_df.select_dtypes(['object']).columns:\n",
    "    print(f'{column}:{combined_df[column].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19321a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect unique values for town, flat_type, storey_range, flat_model\n",
    "columns_to_inspect = ['town', 'flat_type', 'storey_range', 'flat_model']\n",
    "for column in columns_to_inspect:\n",
    "    unique_values = combined_df[column].unique()\n",
    "    print(f'Unique values in {column}:')\n",
    "    print(unique_values)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571eb907",
   "metadata": {},
   "source": [
    "Changing everything to uppercase has reduced much of the differences to a smaller range of values that can be encoded later for modelling. I notice that for flat_type, there are two unique values that likely mean the same thing, multi-generation and multi generation, so I remove the dash to standardise the naming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5fc7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['flat_type'] = np.where(combined_df['flat_type'] == 'MULTI-GENERATION', 'MULTI GENERATION', combined_df['flat_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c55db21",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51855fdf",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daea596",
   "metadata": {},
   "source": [
    "A key limitation of this analysis is that some of the 'month' column refer to approval date while some refer to registration date. However, I cannot know what is the difference in duration between these dates for each datapoint as it is likely to be variable.\n",
    "\n",
    "Hence, I will be treating all of the dates as the same point of time in relation to the sale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa218a33",
   "metadata": {},
   "source": [
    "## Generate remaining_lease column\n",
    "I will recalculate the remaining lease by substracting the month column from the first month of the lease_commence_date.\n",
    "\n",
    "Another key assumption made here is that the lease commence date is the first month of the stated year, which is actually not true as seen later. However, I will treat it as such. This is another limitation in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8051963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify unique values in lease commence date\n",
    "combined_df['lease_commence_date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0773be84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_remaining_lease(lease_commence_date, resale_date):\n",
    "    \"\"\"\n",
    "    Calculate remaining lease years from lease commencement to resale date.\n",
    "    \n",
    "    Args:\n",
    "        lease_commence_date: Year only (e.g., 1995)\n",
    "        resale_date: Year-month format (e.g., \"2020-01\")\n",
    "    \n",
    "    Returns:\n",
    "        Remaining lease in years (float) to 2 decimal places.\n",
    "    \"\"\"\n",
    "    lease_term = 99  # Standard lease term in years\n",
    "            \n",
    "    if isinstance(resale_date, str):\n",
    "        # Handle different formats: \"2020-01\", \"2020-1\"\n",
    "        if '-' in resale_date:\n",
    "            resale_year, resale_month = resale_date.split('-')\n",
    "            resale_year = int(resale_year)\n",
    "            resale_month = int(resale_month)\n",
    "        else:\n",
    "            # only year\n",
    "            resale_year = int(resale_date)\n",
    "            resale_month = 1  # Assume January\n",
    "            \n",
    "    # Calculate elapsed time in years (with month precision)\n",
    "    elapsed_years = resale_year - lease_commence_date\n",
    "    elapsed_months = resale_month - 1  # Start from January of commence year\n",
    "    elapsed_total = elapsed_years + (elapsed_months / 12.0)\n",
    "    \n",
    "    # Calculate remaining lease\n",
    "    remaining_lease = lease_term - elapsed_total\n",
    "    \n",
    "    # Safeguard: ensure remaining lease is not negative\n",
    "    remaining_lease = max(0, remaining_lease)\n",
    "    \n",
    "    return round(remaining_lease, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca54415",
   "metadata": {},
   "source": [
    "First I run this function on the latest dataframe to make sure that the numbers are about the same. I convert the original remaining lease to the same format for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ab1dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lease_format(lease_string):\n",
    "    \"\"\"\n",
    "    Convert remaining lease from \"X years Y months\" format to decimal years\n",
    "    \n",
    "    Args:\n",
    "        lease_string: String like \"61 years 04 months\" or \"85 years 11 months\"\n",
    "    \n",
    "    Returns:\n",
    "        Decimal years (float)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parse the string to extract years and months\n",
    "    lease_string = str(lease_string).lower()\n",
    "    \n",
    "    # Extract years\n",
    "    years = 0\n",
    "    if 'year' in lease_string:\n",
    "        years_part = lease_string.split('year')[0].strip()\n",
    "        years = int(years_part.split()[-1])\n",
    "    \n",
    "    # Extract months  \n",
    "    months = 0\n",
    "    if 'month' in lease_string:\n",
    "        months_part = lease_string.split('year')[-1]\n",
    "        if 'month' in months_part:\n",
    "            months_part = months_part.split('month')[0].strip()\n",
    "            months = int(months_part.split()[-1])\n",
    "    \n",
    "    # Convert to decimal years\n",
    "    return round(years + (months / 12.0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f17b46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_function = price_reg_2017.copy()\n",
    "\n",
    "# Apply conversion to original remaining lease column\n",
    "if 'remaining_lease' in check_function.columns:\n",
    "    check_function['remaining_lease_converted'] = check_function['remaining_lease'].apply(convert_lease_format)\n",
    "\n",
    "check_function['remaining_lease_recalc'] = check_function.apply(\n",
    "    lambda row: calculate_remaining_lease(row['lease_commence_date'], row['month']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Show comparison of different lease calculations\n",
    "comparison_cols = ['lease_commence_date', 'month', 'remaining_lease', 'remaining_lease_converted', 'remaining_lease_recalc']\n",
    "available_cols = [col for col in comparison_cols if col in check_function.columns]\n",
    "display(check_function[available_cols].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1de593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# caculate difference and run basic statistics\n",
    "if 'remaining_lease_converted' in check_function.columns and 'remaining_lease_recalc' in check_function.columns:\n",
    "    check_function['lease_difference'] = check_function['remaining_lease_converted'] - check_function['remaining_lease_recalc']\n",
    "    \n",
    "    difference_stats = {\n",
    "        'mean_difference': check_function['lease_difference'].mean(),\n",
    "        'median_difference': check_function['lease_difference'].median(),\n",
    "        'std_difference': check_function['lease_difference'].std(),\n",
    "        'max_difference': check_function['lease_difference'].max(),\n",
    "        'min_difference': check_function['lease_difference'].min(),\n",
    "    }\n",
    "    \n",
    "    for stat, value in difference_stats.items():\n",
    "        print(f\"{stat}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db21147",
   "metadata": {},
   "source": [
    "Here, we can see that there is actually quite significant variance in the remaining lease calculation and the actual remaining lease, up to a difference of 1.5 years. However, I will still proceed to use my method of recalculating remaining lease for the combined dataframe, as there is no way to determine the actual original purchase date and the sale date, especially for the older years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75be853",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['remaining_lease_recalc'] = combined_df.apply(\n",
    "    lambda row: calculate_remaining_lease(row['lease_commence_date'], row['month']),\n",
    "    axis=1)\n",
    "\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329a7019",
   "metadata": {},
   "source": [
    "## sale_year and sale_month\n",
    "I will also split up the 'month' column, which is actually the sale date, into sale_year and sale_month. This might be a factor that could be related to resale_price, especially if some cooling measures are commonly implemented in certain years or months. There could also be some sort of cyclical effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1dfc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[['sale_year','sale_month']] = combined_df['month'].str.split('-', expand=True)\n",
    "combined_df['sale_year'] = combined_df['sale_year'].astype(int)\n",
    "combined_df['sale_month'] = combined_df['sale_month'].astype(int)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc756b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'month' column\n",
    "combined_df = combined_df.drop(columns=['month'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e724e8",
   "metadata": {},
   "source": [
    "## inspect street_name and block_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd0a901",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(combined_df.value_counts('street_name'))\n",
    "display(combined_df.value_counts('block'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e81f8d",
   "metadata": {},
   "source": [
    "There are 2454 unique block numbers, and these are unlikely to really contribute to sale price on their own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beca9c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp =combined_df.copy()\n",
    "\n",
    "temp['address'] = temp['block'].astype(str) + ' ' + temp['street_name'].astype(str)\n",
    "display(temp.value_counts('address'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e141f889",
   "metadata": {},
   "source": [
    "The large number of combinations make this difficult to use as a feature, especially when many of them only turn up once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de186f4",
   "metadata": {},
   "source": [
    "# Export data for Link Analysis Task II\n",
    "Here I export the datafile to be used for Link Analysis, which is only considering data form 2015 onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909fda81",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_filtered = combined_df[combined_df['sale_year']>=2017]\n",
    "combined_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078f4af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_filtered.to_csv(raw_data_folder / 'combined_hdb_resale_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796a21fd",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddf5ff4",
   "metadata": {},
   "source": [
    "## Categorical columns\n",
    "I plot box and whisker plot for categorical columns agaisnt sale price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c50633",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_plot = ['town', 'flat_type', 'storey_range', 'flat_model', 'sale_year', 'sale_month', 'lease_commence_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc872fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the subplot layout\n",
    "n_cols = 3\n",
    "n_rows = 3\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 6*n_rows))\n",
    "axes = axes.flatten()  # Flatten for easier indexing\n",
    "\n",
    "for i, column in enumerate(columns_to_plot):\n",
    "    if i < len(axes):\n",
    "        # Sort categories alphabetically for each column\n",
    "        sorted_categories = sorted(combined_df[column].unique())\n",
    "        \n",
    "        sns.boxplot(data=combined_df, x=column, y='resale_price', ax=axes[i], order=sorted_categories)\n",
    "        \n",
    "        axes[i].set_title(f'Resale Price by {column.replace(\"_\", \" \").title()}', fontsize=12, fontweight='bold')\n",
    "        axes[i].set_xlabel(column.replace(\"_\", \" \").title(), fontsize=10)\n",
    "        axes[i].set_ylabel('Resale Price (SGD)', fontsize=10)\n",
    "\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "        for label in axes[i].get_xticklabels():\n",
    "            label.set_horizontalalignment('right')\n",
    "            label.set_rotation_mode('anchor')\n",
    "\n",
    "        # Format y-axis to show prices in thousands\n",
    "        axes[i].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1000:.0f}K'))\n",
    "    \n",
    "# Hide any unused subplots\n",
    "for i in range(len(columns_to_plot), len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Distribution of Resale Prices by Different Features', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c101de",
   "metadata": {},
   "source": [
    "There is clearly some relationships between the different categorical features and and resale price, with the clearest being an increasing trend with flat type and storey range. Prices also increase over the years. The smallest impact is sale_month, but there is some minor drift in the median."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0ec1dd",
   "metadata": {},
   "source": [
    "## Numerical columns\n",
    "For numerical columns, I do a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5f133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_scatter = [\n",
    "    'floor_area_sqm', \n",
    "    'remaining_lease_recalc'\n",
    "    ]\n",
    "\n",
    "n_cols = 2\n",
    "n_rows = 1\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5))\n",
    "\n",
    "for i, column in enumerate(columns_to_scatter):\n",
    "    x_data = combined_df[column]\n",
    "    \n",
    "    axes[i].scatter(x_data, combined_df['resale_price'], alpha=0.5, s=5)\n",
    "    \n",
    "    axes[i].set_title(f'Resale Price vs {column.replace(\"_\", \" \").title()}', fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xlabel(column.replace(\"_\", \" \").title(), fontsize=10)\n",
    "    axes[i].set_ylabel('Resale Price (SGD)', fontsize=10)\n",
    "    \n",
    "    # Format y-axis to show prices in thousands\n",
    "    axes[i].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1000:.0f}K'))\n",
    "    \n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Resale Price vs Continuous Features', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14048c06",
   "metadata": {},
   "source": [
    "There appears to be some upward trend with resale price and floor area sqm, and potentially some trend with resale price and remaining lease, but the latter is less clear, except that resale price decreases a lot when there is little lease remaining, which makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ac7aa9",
   "metadata": {},
   "source": [
    "# Model Training with Optuna Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf98c9",
   "metadata": {},
   "source": [
    "A predictive model for housing prices needs to predict prices for future data, so the training set should use data from the older years, and the test set should be newer data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f4f781",
   "metadata": {},
   "source": [
    "## Train test split\n",
    "90-10 split is prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0566ae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure chronological order\n",
    "df = combined_df.sort_values(['sale_year', 'sale_month']).reset_index(drop=True)\n",
    "\n",
    "# Define split points\n",
    "test_idx = int(len(df) * 0.90)\n",
    "X_train = df.iloc[:test_idx].copy().drop(columns=['resale_price'])\n",
    "y_train = df.iloc[:test_idx]['resale_price']\n",
    "\n",
    "X_test = df.iloc[test_idx:].copy().drop(columns=['resale_price'])\n",
    "y_test = df.iloc[test_idx:]['resale_price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e20231",
   "metadata": {},
   "source": [
    "I also extract the letter after the block number and change the remaining to integer, which allows the model to treat it as a numerical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443a7e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_block_column(df):\n",
    "    # 1. Extract the numeric part (e.g., '123A' -> 123)\n",
    "    df['block_num'] = df['block'].str.extract('(\\d+)').astype(int)\n",
    "    \n",
    "    # 2. Create a flag for suffixes (e.g., '123A' -> 1, '123' -> 0)\n",
    "    # This helps the model identify special blocks like annexes or SERS replacements\n",
    "    df['block_has_letter'] = df['block'].str.contains('[a-zA-Z]').astype(int)\n",
    "    \n",
    "    df = df.drop(columns=['block'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply to all your sets\n",
    "X_train = clean_block_column(X_train)\n",
    "X_test = clean_block_column(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c704bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889d18ca",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af19619f",
   "metadata": {},
   "source": [
    "I perform One Hot Encoding on town and flat_model, as there is no clear ordering for these categorical values. For flat_type and storey_range, there is a clear order so I perform ordinal encoding. I perform target_encoding for street_name, as there may be premium streets. However, I only use the train data to encode so there is no data leakage. The TargetEncoder also has default smoothing, and this is performed inside cross validation to minimise risk of data leakage. The encoders are also fitted on train data, and used to transform val and test data (without refitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1df5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Ordinal Orders\n",
    "flat_type_order = ['1 ROOM', '2 ROOM', '3 ROOM', '4 ROOM', '5 ROOM', 'EXECUTIVE', 'MULTI GENERATION']\n",
    "storey_order = sorted(X_train['storey_range'].unique(), key=lambda x: int(x.split(' ')[0]))\n",
    "\n",
    "# 2. Define Refined Column Groups\n",
    "# Note: 'block' is dropped from target_enc_cols to reduce noise\n",
    "numerical_cols = ['floor_area_sqm', 'remaining_lease_recalc', 'block_num']\n",
    "ordinal_cols = ['flat_type', 'storey_range']\n",
    "ohe_cols = ['town', 'flat_model']\n",
    "target_enc_cols = ['street_name'] \n",
    "passthrough_cols = ['lease_commence_date', 'sale_year','sale_month', 'block_has_letter']\n",
    "\n",
    "# 3. Build the Hybrid Preprocessor using sklearn's TargetEncoder\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('ord', OrdinalEncoder(categories=[flat_type_order, storey_order], \n",
    "                               handle_unknown='use_encoded_value', \n",
    "                               unknown_value=-1), ordinal_cols),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False), ohe_cols),\n",
    "        ('target', TargetEncoder(target_type='continuous'), target_enc_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8ce090",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_check = preprocessor.fit_transform(X_train, y_train)\n",
    "print(X_check.shape)\n",
    "X_check.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82acb7f9",
   "metadata": {},
   "source": [
    "## Transformation of y to log scale\n",
    "y is transformed to log_scale because mistakes in lower-priced housing is as important as mistakes in high-priced housing. We want to optimise for the % error in price, rather than raw values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6d66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform target to log scale\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_test_log = np.log1p(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fc51ee",
   "metadata": {},
   "source": [
    "## Run Optimisation\n",
    "TimeSeriesSplit is used so that selected pipelines are able to generalize on future data. Sample Weights are also calculated to make sure that less frequent flat types are not overlooked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f15a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'tree_method': 'hist',\n",
    "        'device': 'cpu', # ensure use of CPU\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "        'n_estimators': 2000, \n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-2, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-2, 10.0, log=True),\n",
    "    }\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    cv_scores = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(X_train):\n",
    "        X_tr, X_va = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_tr_log, y_va_log = y_train_log.iloc[train_index], y_train_log.iloc[val_index]\n",
    "\n",
    "        weights_tr = compute_sample_weight(class_weight='balanced', y=X_tr['flat_type'])\n",
    "\n",
    "        # Process features\n",
    "        X_tr_proc = preprocessor.fit_transform(X_tr, y_tr_log)\n",
    "        X_va_proc = preprocessor.transform(X_va)\n",
    "\n",
    "        model = xgboost.XGBRegressor(**params, early_stopping_rounds=50)\n",
    "        \n",
    "        # Fit using sample weights to protect less common flat_types\n",
    "        model.fit(X_tr_proc, y_tr_log, \n",
    "                  sample_weight=weights_tr,\n",
    "                  eval_set=[(X_va_proc, y_va_log)], \n",
    "                  verbose=False)\n",
    "\n",
    "        # Predict and transform back from log to original scale for RMSE\n",
    "        preds_log = model.predict(X_va_proc)\n",
    "        preds = np.expm1(preds_log)\n",
    "        actuals = np.expm1(y_va_log)\n",
    "        \n",
    "        rmse = root_mean_squared_error(actuals, preds)\n",
    "        cv_scores.append(rmse)\n",
    "\n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a94cf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run Optuna study\n",
    "sampler = optuna.samplers.TPESampler(seed=42, n_startup_trials=10)\n",
    "TOTAL_TRIALS_GOAL = 50\n",
    "\n",
    "# 2. Load or create the study\n",
    "study = optuna.create_study(\n",
    "    direction='minimize', \n",
    "    sampler=sampler, \n",
    "    study_name=\"exp1\", \n",
    "    storage=\"sqlite:///study_xgboost.db\", \n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "# 3. Calculate how many are left to run\n",
    "completed_trials = len(study.trials)\n",
    "trials_to_run = max(0, TOTAL_TRIALS_GOAL - completed_trials)\n",
    "\n",
    "if trials_to_run > 0:\n",
    "    print(f\"Study '{study.study_name}' has {completed_trials} trials. Running {trials_to_run} more...\")\n",
    "    study.optimize(objective, n_trials=trials_to_run, show_progress_bar=True)\n",
    "else:\n",
    "    print(f\"Study already reached the goal of {TOTAL_TRIALS_GOAL} trials.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2d09a5",
   "metadata": {},
   "source": [
    "# Train Final Model and Inspect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288cf703",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.load_study(\n",
    "    study_name=\"exp1\",\n",
    "    storage=\"sqlite:///study_xgboost.db\"\n",
    ")\n",
    "\n",
    "# Get best parameters\n",
    "best_params = study.best_params\n",
    "best_rmse = study.best_value\n",
    "\n",
    "print(f\"\\nBest RMSE: ${best_rmse:,.2f}\")\n",
    "print(f\"Best parameters:\")\n",
    "for param, value in best_params.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {param}: {value:.6f}\")\n",
    "    else:\n",
    "        print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ef105a",
   "metadata": {},
   "source": [
    "For fitting of final model, I use the final Time Series Split to obtain the validation set, and fit the model on the rest of the training data. This also helps to prevent over fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc1bcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best parameters on X_train\n",
    "final_params = best_params.copy()\n",
    "final_params['random_state'] = 42\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=8) # approximately 10% of original data\n",
    "# Get the indices for the very last split (the most recent data)\n",
    "train_index, val_index = list(tscv.split(X_train))[-1]\n",
    "\n",
    "X_tr, X_va = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "y_tr_log, y_va_log = y_train_log.iloc[train_index], y_train_log.iloc[val_index]\n",
    "\n",
    "# Recalculate weights ONLY for the training portion\n",
    "weights_tr = compute_sample_weight(class_weight='balanced', y=X_tr['flat_type'])\n",
    "\n",
    "# Process features separately to avoid leakage \n",
    "X_tr_proc = preprocessor.fit_transform(X_tr, y_tr_log)\n",
    "X_va_proc = preprocessor.transform(X_va)\n",
    "\n",
    "final_model = xgboost.XGBRegressor(**best_params, n_estimators=5000,early_stopping_rounds=50,)\n",
    "final_model.fit(\n",
    "    X_tr_proc, y_tr_log,\n",
    "    sample_weight=weights_tr,\n",
    "    eval_set=[(X_va_proc, y_va_log)],\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0b2bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on val and blind test sets\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "y_va_pred = np.expm1(final_model.predict(X_va_proc))\n",
    "y_test_pred = np.expm1(final_model.predict(X_test_preprocessed))\n",
    "\n",
    "val_rmse = root_mean_squared_error(np.expm1(y_va_log), y_va_pred)\n",
    "val_r2 = r2_score(np.expm1(y_va_log), y_va_pred)\n",
    "\n",
    "test_rmse = root_mean_squared_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\nFinal Model Performance:\")\n",
    "print(f\"\\nValidation Set:\")\n",
    "print(f\"  RMSE: ${val_rmse:,.2f}\")\n",
    "print(f\"  R² Score: {val_r2:.4f}\")\n",
    "print(f\"\\nTest Set (10%, Blind):\")\n",
    "print(f\"  RMSE: ${test_rmse:,.2f}\")\n",
    "print(f\"  R² Score: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b181a044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a results dataframe for the test set\n",
    "results = X_test.copy()\n",
    "results['actual'] = y_test\n",
    "results['predicted'] = np.expm1(final_model.predict(X_test_preprocessed))\n",
    "results['error'] = results['predicted'] - results['actual']\n",
    "results['abs_error'] = results['error'].abs()\n",
    "\n",
    "# Calculate RMSE by Town\n",
    "town_rmse = results.groupby('town').apply(\n",
    "    lambda x: root_mean_squared_error(x['actual'], x['predicted'])\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "# Calculate Mean Absolute Percentage Error (MAPE) by Flat Type\n",
    "flat_type_mape = results.groupby('flat_type').apply(\n",
    "    lambda x: (x['abs_error'] / x['actual']).mean() * 100\n",
    ")\n",
    "\n",
    "print(\"Top 5 Towns with Highest Error (RMSE):\")\n",
    "print(town_rmse.head(20))\n",
    "print(\"\\nAverage Error % by Flat Type:\")\n",
    "print(flat_type_mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff5b260",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5416258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_check = preprocessor.transform(X_train)\n",
    "final_model.get_booster().feature_names = X_check.columns.tolist()\n",
    "\n",
    "# Get the importance (Gain)\n",
    "importance_dict = final_model.get_booster().get_score(importance_type='gain')\n",
    "\n",
    "# plotting\n",
    "importance_df = pd.Series(importance_dict).sort_values(ascending=True)\n",
    "\n",
    "# 4. Plot the top 20 features\n",
    "plt.figure(figsize=(10, 10))\n",
    "importance_df.tail(20).plot(kind='barh', color='skyblue')\n",
    "plt.xlabel(\"Average Gain per Split\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Top 20 Features by Gain (Predicting Log Resale Price)\")\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2298a23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the model's predictions\n",
    "explainer = shap.TreeExplainer(final_model)\n",
    "shap_values = explainer.shap_values(X_test_preprocessed)\n",
    "\n",
    "# Summary plot\n",
    "shap.summary_plot(shap_values, X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8174afe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "htx_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
